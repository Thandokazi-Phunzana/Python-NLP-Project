{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"IrGmoy-dwa-g","colab":{"base_uri":"https://localhost:8080/","height":419},"outputId":"f2e0e914-93bb-4e4d-f2da-20f9ca38fec0","executionInfo":{"status":"error","timestamp":1769997117479,"user_tz":-120,"elapsed":11037,"user":{"displayName":"Thandokazi Phunzana","userId":"03502270933352388177"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Articles'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (22/22), done.\u001b[K\n","remote: Total 24 (delta 2), reused 23 (delta 1), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (24/24), 20.34 KiB | 10.17 MiB/s, done.\n","Resolving deltas: 100% (2/2), done.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/Articles/Music.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3382854264.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Open an article in read mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Articles/Music.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Read the entire contents of the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Articles/Music.txt'"]}],"source":["#clone the articles used in the model from a github repository\n","!git clone https://github.com/MhlongoCB/Articles.git\n","\n","#preprocessing\n","import nltk\n","import string\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","#weighted graph\n","import networkx as nx\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#create a pgn file to represent our graph\n","import matplotlib.pyplot as plt\n","\n","# Open an article in read mode\n","with open('/content/Articles/Music.txt', 'r') as file:\n","    # Read the entire contents of the article\n","    text = file.read()\n","\n","# Tokenization\n","sentences = sent_tokenize(text)\n","\n","# Preprocessing\n","stop_words = set(stopwords.words('english'))\n","punctuation = set(string.punctuation)\n","lem = WordNetLemmatizer()\n","\n","def preprocess_word(word):\n","    if word in stop_words or word in punctuation:\n","        return None\n","    return lem.lemmatize(word,\"v\")\n","\n","def preprocess_sentence(sentence):\n","    # Tokenize the sentence into words\n","    words = word_tokenize(sentence.lower())\n","\n","    # Apply preprocessing to each word\n","    preprocessed_words = []\n","    for word in words:\n","        preprocessed_word = preprocess_word(word)\n","        if preprocessed_word is not None:\n","            preprocessed_words.append(preprocessed_word)\n","\n","    # Join the preprocessed words back into a single string\n","    preprocessed_sentence = ' '.join(preprocessed_words)\n","\n","    return preprocessed_sentence\n","\n","# Preprocess sentences and create corpus\n","corpus = [preprocess_sentence(sentence) for sentence in sentences]\n","\n","# Cosine similarity\n","def cosine_sim():\n","  # Calculate TF-IDF vectors\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform(corpus)\n","\n","    # Calculate cosine similarity\n","    cosine_sim = np.dot(tfidf_matrix, tfidf_matrix.T).toarray()\n","    return cosine_sim\n","\n","  # Build graph\n","def graph():\n","  G = nx.Graph()\n","\n","  # Add nodes\n","  for i, sentence in enumerate(sentences):\n","    G.add_node(i, sentence=sentence)\n","\n","  # Add edges\n","  for i in range(len(sentences)):\n","    for j in range(i+1, len(sentences)):\n","        sim = cosine_sim()[i][j]\n","        G.add_edge(i, j, weight=sim)\n","  return G\n","\n","G = graph()\n","\n","# Iterate through edges and remove weak ones\n","def threshold():\n","  threshold = 0.2\n","  for edge in list(G.edges(data=True)):\n","      if edge[2]['weight'] < threshold:\n","          G.remove_edge(edge[0], edge[1])\n","  return None\n","\n","# threshold()\n","\n","\n","plt.clf()\n","nx.draw(G, with_labels = True)\n","plt.savefig('graph.png')\n","\n","def page_rank():\n","\n","  # Calculate PageRank scores\n","  pagerank_scores = nx.pagerank(G, weight='weight')\n","\n","  # Sort nodes by PageRank score in descending order\n","  sorted_nodes = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","  top_sentences = sorted_nodes[:5]  # Select the top 5 sentences\n","  return top_sentences\n","\n","def main():\n","    with open('output.txt', 'w') as file:\n","    # Write content to the file\n","      for i, (node_id, score) in enumerate(page_rank()):\n","        file.write(sentences[node_id])\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}